{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework2_AishwaryaChoukekar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orvgBiZaAS-S"
      },
      "source": [
        "# Homework: classify the origin of names using a character-level RNN\n",
        "\n",
        "In this homework we will use an rnn-based model to perform classification. The goal is threefold:\n",
        "\n",
        "1. Get more hands on with the preprocessing needed to perform text classification from A to Z. No preprocessing is done for you!\n",
        "2. Use embeddings and RNNs in conjunction at the character level to perform classification.\n",
        "3. Write a function that takes as input a string, and outputs the name of the predicted class.\n",
        "\n",
        "However, here are guidelines to help you through all the steps:\n",
        "\n",
        "1. Figure out the number of classes, and map the classes to integers (or one-hot vectors). This is needed for fitting the model and training it to do classification.\n",
        "2. Use the keras tokenizer at the character level to tokenize your input into integer sequences.\n",
        "3. Pad your sequences using the keras preprocessing tools.\n",
        "4. Build a model that uses, minimally, an embedding layer, an RNN (of your choice) and a dense layer to output the logits or probabilities for the target classes (name origins).\n",
        "5. Fit the model and evaluate on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries and getting data"
      ],
      "metadata": {
        "id": "jWSZrB4zdcyN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxpRYUBTCANr"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, LSTM\n",
        "from sklearn import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF1BqaO7-A2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da480ece-a658-4e62-d5a3-b2ae8c4c9bee"
      },
      "source": [
        "# Download the data\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-14 22:03:24--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.64.174.109, 18.64.174.23, 18.64.174.42, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.64.174.109|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-14 22:03:24 (21.6 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As part of pre-processing we are replacing certain characters and punctuations. But we will be preserving space, inverted comma and non-english alphbets to preserve essence of certain languages."
      ],
      "metadata": {
        "id": "yAksCeq9ks1N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnj5MDEJ-u-q"
      },
      "source": [
        "data = []\n",
        "for filename in glob('data/names/*.txt'):\n",
        "  origin = filename.split('/')[-1].split('.txt')[0]\n",
        "  names = open(filename).readlines()\n",
        "  for name in names:\n",
        "    name = name.replace(u'\\xa0', ' ')\n",
        "    data.append((re.sub(r'[/\\\\,\\-?#@:0-9]', '', name.strip()), origin)) #keeping only alphabets as part of pre-processing along with stripping.\n",
        "\n",
        "names, origins = zip(*data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining encoders. Label encoder is used first to get different origins and then these different classes are encoded with one hot encoding."
      ],
      "metadata": {
        "id": "XZuHzqMbeWjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#label encoding different language classes\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "enc = OneHotEncoder(sparse=False)"
      ],
      "metadata": {
        "id": "F3ewMl2svUg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "origin_int = label_encoder.fit_transform(origins)\n",
        "origin_label = enc.fit_transform(origin_int.reshape(-1, 1))\n",
        "\n",
        "#This is how each row would look after encoding is completed\n",
        "origin_label[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKhUvqOUmfre",
        "outputId": "34da7749-e5a1-4ede-b2bc-2a69f2b2f589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Numbere of language classes\n",
        "lang_classes = label_encoder.classes_\n",
        "print(\"The number of unique language classes: \", len(lang_classes))\n",
        "print(\"\\nThe different languages found are as below:\\n\",lang_classes)\n",
        "\n",
        "#18 languages detected"
      ],
      "metadata": {
        "id": "ES5GYhKZOh_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fefad59-72fa-4277-8a39-27e5d8b6a98f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique language classes:  18\n",
            "\n",
            "The different languages found are as below:\n",
            " ['Arabic' 'Chinese' 'Czech' 'Dutch' 'English' 'French' 'German' 'Greek'\n",
            " 'Irish' 'Italian' 'Japanese' 'Korean' 'Polish' 'Portuguese' 'Russian'\n",
            " 'Scottish' 'Spanish' 'Vietnamese']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Char-Level Tokenizing"
      ],
      "metadata": {
        "id": "j3s1nkk8e9J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t  = Tokenizer(char_level=True)"
      ],
      "metadata": {
        "id": "WDe793XdcHN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = []\n",
        "for name in names:\n",
        "  t.fit_on_texts(name)\n",
        "\n",
        "print(\"Count of characters:\",t.word_counts)"
      ],
      "metadata": {
        "id": "S4B7yVWtdoRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bbcbc0-263e-4b3e-b7b4-0e9fe5dbb25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of characters: OrderedDict([('a', 16516), ('h', 7688), ('n', 9961), ('b', 3657), ('i', 10422), ('k', 6922), ('g', 3217), ('y', 3619), ('o', 11106), ('c', 3070), ('e', 10764), ('u', 4720), ('w', 1127), ('l', 6713), ('j', 1351), ('m', 4351), ('p', 1711), ('r', 8262), ('s', 7985), ('t', 5956), ('d', 3899), ('x', 73), ('f', 1778), ('v', 6315), ('z', 1932), (' ', 116), ('q', 98), ('ó', 13), ('á', 13), ('ú', 7), ('í', 14), ('é', 23), (\"'\", 87), ('à', 10), ('ñ', 6), ('ż', 2), ('ń', 1), ('ł', 1), ('ś', 3), ('ą', 1), ('ò', 3), ('ù', 1), ('ì', 1), ('è', 2), ('ã', 2), ('õ', 1), ('ü', 11), ('ä', 13), ('ö', 24), ('ß', 9), ('ê', 1), ('ç', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that we have dictionary of our characters, we can get sequences for each character as shown below."
      ],
      "metadata": {
        "id": "eQBbtM9afFG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = t.texts_to_sequences(names)\n",
        "data[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYfNZmFcgQi2",
        "outputId": "7f3ba105-7e64-44b4-f9c1-e748706eee89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 8, 5], [16, 1, 4, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_data = tf.keras.preprocessing.sequence.pad_sequences(data, padding='post')\n",
        "pad_data[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ikR45YhCaJ",
        "outputId": "27aff934-90cd-492a-cbfc-8d8b96dfaa30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  8,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0],\n",
              "       [16,  1,  4,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data into training set and the rest of the set\n",
        "X_train,X_rest,y_train,y_rest = train_test_split(pad_data, origin_label, test_size=0.2, random_state=4)\n",
        "\n",
        "#splitting the rest set further into test and validation set\n",
        "X_valid,X_test,y_valid,y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=4)"
      ],
      "metadata": {
        "id": "MENZyJBivYA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YITmeoMT673X",
        "outputId": "b0b58248-9be1-421e-a1a8-6ab866a14fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  1,  8, 14,  1,  5,  2, 11,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0],\n",
              "       [22,  4, 21, 21,  3,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqkNBSCnpjq7",
        "outputId": "eee33824-4ae7-4000-8732-75ba24f11aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDZyR2Hkq258",
        "outputId": "bef72645-392b-4f13-df36-a9c1b0b758b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16059, 19)\n",
            "(16059, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUhDpvskAHZ1"
      },
      "source": [
        "### Building Model\n",
        "\n",
        "Model is built using Embedding layer, LSTM layers and Dense layers with Adam as optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ux9M4DV-A5s"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(X_train)\n",
        "embed_size = 25"
      ],
      "metadata": {
        "id": "HvvWgPaJWLNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, embed_size, input_shape=[None], mask_zero=True))\n",
        "model.add(LSTM(512, return_sequences=True, input_shape=X_train.shape[1:]))\n",
        "model.add(LSTM(128, dropout=0.5))\n",
        "model.add(Dense(18, activation = 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cGYKUgAiWh38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dde965-b6df-4df4-ad10-36dfbba7c629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 25)          401475    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 512)         1101824   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               328192    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 18)                2322      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,833,813\n",
            "Trainable params: 1,833,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ],
      "metadata": {
        "id": "MeaGERp3mP9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid),callbacks=[ResetStatesCallback()])"
      ],
      "metadata": {
        "id": "Op0bZ0u5-9L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff294a1-dc24-4dad-89e6-aa00bea4a71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "502/502 [==============================] - 179s 340ms/step - loss: 1.5650 - accuracy: 0.5509 - val_loss: 1.3261 - val_accuracy: 0.6089\n",
            "Epoch 2/10\n",
            "502/502 [==============================] - 167s 332ms/step - loss: 1.1833 - accuracy: 0.6533 - val_loss: 1.0503 - val_accuracy: 0.6876\n",
            "Epoch 3/10\n",
            "502/502 [==============================] - 166s 330ms/step - loss: 0.9587 - accuracy: 0.7129 - val_loss: 0.9191 - val_accuracy: 0.7210\n",
            "Epoch 4/10\n",
            "502/502 [==============================] - 167s 333ms/step - loss: 0.8367 - accuracy: 0.7499 - val_loss: 0.8284 - val_accuracy: 0.7424\n",
            "Epoch 5/10\n",
            "502/502 [==============================] - 169s 337ms/step - loss: 0.7274 - accuracy: 0.7777 - val_loss: 0.7301 - val_accuracy: 0.7783\n",
            "Epoch 6/10\n",
            "502/502 [==============================] - 167s 332ms/step - loss: 0.6424 - accuracy: 0.8009 - val_loss: 0.7004 - val_accuracy: 0.7813\n",
            "Epoch 7/10\n",
            "502/502 [==============================] - 165s 329ms/step - loss: 0.5701 - accuracy: 0.8199 - val_loss: 0.6553 - val_accuracy: 0.7867\n",
            "Epoch 8/10\n",
            "502/502 [==============================] - 166s 330ms/step - loss: 0.5066 - accuracy: 0.8360 - val_loss: 0.6319 - val_accuracy: 0.8032\n",
            "Epoch 9/10\n",
            "502/502 [==============================] - 161s 322ms/step - loss: 0.4468 - accuracy: 0.8574 - val_loss: 0.6196 - val_accuracy: 0.8042\n",
            "Epoch 10/10\n",
            "502/502 [==============================] - 173s 344ms/step - loss: 0.3910 - accuracy: 0.8720 - val_loss: 0.6097 - val_accuracy: 0.8107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on test set\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jFyL0run-Kt",
        "outputId": "02289623-fc56-4d81-f189-087d5dc296cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 5s 71ms/step - loss: 0.5771 - accuracy: 0.8297\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5771210193634033, 0.8296812772750854]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting predictions"
      ],
      "metadata": {
        "id": "RIKyclLml-6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Just for reference, re-printing our list of language class in which order label encoder assigned the labels\n",
        "lang_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOc5Wgphoqpn",
        "outputId": "5b951b1a-d65c-48e3-ed56-e34926e37ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French',\n",
              "       'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean',\n",
              "       'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish',\n",
              "       'Vietnamese'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xESBh1qu6kUo"
      },
      "source": [
        "def predict_origin(input_name):\n",
        "  #pre-process input string\n",
        "  assert isinstance(input_name, str)\n",
        "  input_name = input_name.replace(u'\\xa0', ' ')\n",
        "  input_name = re.sub(r'[/\\\\,\\-?#@:0-9]', '', input_name.strip())\n",
        "\n",
        "  #get tokens for input string and flatten list of lists to list for padding\n",
        "  input = t.texts_to_sequences(input_name)\n",
        "  flat_list = [item for sublist in input for item in sublist]\n",
        "\n",
        "  #padding based on length of trained model\n",
        "  input = tf.keras.preprocessing.sequence.pad_sequences([flat_list], padding='post', maxlen=20)\n",
        "\n",
        "  #get prediction based on trained model\n",
        "  output = model.predict(input) \n",
        "\n",
        "  #get highest probability amongst 18 outputs and its index\n",
        "  max_val = np.amax(output, axis=1)\n",
        "  max_idx = np.argmax(output, axis=1)\n",
        "\n",
        "  #return language class and its probability\n",
        "  the_origin = lang_classes[max_idx] \n",
        "  return the_origin, max_val*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on random last names"
      ],
      "metadata": {
        "id": "2kWM4idRt6HK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbriVUIp6vJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97e3ac3-ecfb-4714-9e31-c41ccfc93956"
      },
      "source": [
        "lang, prob = predict_origin(\"Cha\")\n",
        "print(\"The name is {} predicted with probability of {}\".format(lang, prob))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The name is ['Vietnamese'] predicted with probability of [32.05766]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang, prob = predict_origin(\"Schmidt\")\n",
        "print(\"The name is {} predicted with probability of {}\".format(lang, prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfT44ExAjTNE",
        "outputId": "00f38216-af93-43da-f816-3ee95e1de83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The name is ['German'] predicted with probability of [59.0673]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang, prob = predict_origin(\"Trump\")\n",
        "print(\"The name is {} predicted with probability of {}\".format(lang, prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vq7I2rglonc",
        "outputId": "d9514269-ea99-49b1-c5eb-ffb451f43f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The name is ['English'] predicted with probability of [72.211655]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rMSZ9J5Ru4kL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}